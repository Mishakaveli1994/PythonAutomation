{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<style>\n",
    "div.heading{\n",
    "    padding: 0 10%;\n",
    "    text-align:center;\n",
    "    }\n",
    "\n",
    "p.text{\n",
    "    text-align:center;\n",
    "    padding: 0 10%;\n",
    "\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# <p class=\"text\">Python for Automation - Lesson 8</p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<div class=\"heading\">\n",
    "    <ul style=\"list-style-type:none\">\n",
    "        <li><b>Lesson 8 Structure:</b></li>\n",
    "        <li>Parsing HTML</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <p class=\"text\">What is HTML?</p>\n",
    "\n",
    "<p class=\"text\"><b>H</b>yper<b>T</b>ext <b>M</b>arkup <b>L</b>anguage or <b>HTML</b> is the standard markup language for documents designed to be displayed in a web browser. It defines the content and structure of web content. It is often assisted by technologies such as <b>C</b>ascading <b>S</b>tyle <b>S</b>heets (<b>CSS</b>) and scripting languages such as JavaScript.\n",
    "\n",
    "Web browsers receive HTML documents from a web server or from local storage and render the documents into multimedia web pages. HTML describes the structure of a web page semantically and originally included cues for its appearance. HTML files by nature have a nested structure</p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <p class=\"text\">Example HTML</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "example_html = \"\"\"<!DOCTYPE html>\n",
    "<html>\n",
    "<body>\n",
    "\n",
    "<h1>My First Heading</h1>\n",
    "\n",
    "<p>My first paragraph.</p>\n",
    "\n",
    "</body>\n",
    "</html>\"\"\"\n",
    "print(example_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation = \"\"\"Example Explained\n",
    "The <!DOCTYPE html> declaration defines that this document is an HTML5 document\n",
    "\n",
    "The <html> element is the root element of an HTML page\n",
    "\n",
    "The <head> element contains meta information about the HTML page\n",
    "\n",
    "The <title> element specifies a title for the HTML page (which is shown in the browser's title bar or in the page's tab)\n",
    "\n",
    "The <body> element defines the document's body, and is a container for all the visible contents, such as headings, paragraphs, images, hyperlinks, tables, lists, etc.\n",
    "\n",
    "The <h1> element defines a large heading\n",
    "\n",
    "The <p> element defines a paragraph</p>\"\"\"\n",
    "print(explanation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p class=\"text\"><b>There are numerous uses for parsing HTML files - scraping sites (getting their content and organizing it in a useful fashion), analyzing data retrieved from websites without a REST interface and automating QA tests related to webpage content.</b></p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <p class=\"text\">Parsing HTML</p>\n",
    "\n",
    "<p class=\"text\">There are numerous libraries that can be used to strictly parse HTML content. I'm going to show you 2 of the most prevelent ones - <code>Beautiful Soup</code> and <code>Selenium</code>.</p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <p class=\"text\">Beautiful Soup</p>\n",
    "\n",
    "<p class=\"text\">Beautiful Soup is a Python package for parsing HTML and XML documents, including those with malformed markup. It creates a parse tree for documents that can be used to extract data from HTML, which is useful for web scraping.</p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <p class=\"text\">Installing Beautiful Soup: <code>python -m pip install beautifulsoup4</code></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample use\n",
    "\n",
    "# We import the BeautifulSoup class from bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "url = requests.get(\"https://www.mobile.bg/obiavi/avtomobili-dzhipove/honda/accord\")\n",
    "url.encoding = 'windows-1251' #  Needed as the site contains Unicode characters\n",
    "htmltext = url.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show part of received HTML\n",
    "htmltext[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p class=\"text\">When we have a site in mind, the proper way to scrape it is to familiarize with the part of HTML code that we want to review and then extract only the needed parts with Beautiful Soup.</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In our example, we want to take all titles, pricings and urls in site of the cars here\n",
    "\n",
    "# This is how we pass the .html to BeautifulSoup and create a instance that we can manipulate\n",
    "soup = BeautifulSoup(htmltext) # We can also pass a string, taken from a .html file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are searching for the span cell (<span>) tag with class 'price' to get all prices\n",
    "prices = soup.find_all('span', class_='price')\n",
    "print(prices[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices = [int(price.text[:-4].replace(' ', '')) for price in prices] # We are only interest in the actual prices string, not the whole html structure\n",
    "print(prices[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also need the listings title, upon investigation it's contained in the anchor tag (<a>) with class mmmL\n",
    "# Lucky for us, also the url for the offer is contained in the same tag\n",
    "listing_names_raw = soup.find_all('a', class_='mmmL')\n",
    "print(listing_names_raw[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get title text\n",
    "listing_names = [listing.text for listing in listing_names_raw]\n",
    "print(listing_names[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get listing url\n",
    "listing_url = [listing['href'][2:] for listing in listing_names_raw]\n",
    "print(listing_url[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now combine the both to form a complete entry\n",
    "title_price_url = [list(i) for i in zip(listing_names, prices, listing_url)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_price_url[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or we can for example create a dataframe for more appealing inspection\n",
    "import pandas as pd\n",
    "\n",
    "# Set pandas options, so it does not truncate columns\n",
    "pd.set_option('display.max_column', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_seq_items', None)\n",
    "pd.set_option('display.max_colwidth', 500)\n",
    "pd.set_option('expand_frame_repr', True)\n",
    "\n",
    "offers_df = pd.DataFrame(title_price_url, columns=['Car Title', 'Pricing', 'Listing URL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offers_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <p class=\"text\">Selenium</p>\n",
    "\n",
    "<p class=\"text\">Selenium of Selenium Web Driver how it's officially called, is a module that we can use to simulate human interaction with a website. Below we are going to extend the above example by first going over all pages, not just the first one, adding pictures and car data to the dataframe. Selenium is often used in tandem with Beautiful Soup - we use selenium for interaction with the website and Beautiful Soup for scraping the content.</p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <p class=\"text\">Installing Selenium: <code>python -m pip install selenium</code></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample use\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "# Initialize a Firefox driver\n",
    "driver = webdriver.Firefox()\n",
    "\n",
    "# Go to a specific site\n",
    "driver.get(\"http://www.python.org\")\n",
    "\n",
    "# Check if string is in title of said page\n",
    "assert \"Python\" in driver.title\n",
    "\n",
    "# Find the query bar\n",
    "elem = driver.find_element(By.NAME, \"q\")\n",
    "time.sleep(1)\n",
    "\n",
    "elem.clear()\n",
    "# Write the search term pycon\n",
    "elem.send_keys(\"pycon\")\n",
    "time.sleep(1)\n",
    "\n",
    "# Press go\n",
    "elem.send_keys(Keys.RETURN)\n",
    "time.sleep(1)\n",
    "\n",
    "# Verify that there are displayed results\n",
    "assert \"No results found.\" not in driver.page_source\n",
    "# driver.close() - We want to close it by ourselves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_cars(soup:BeautifulSoup) -> list:\n",
    "    \"\"\"\n",
    "    Scrape car entries from page\n",
    "\n",
    "    : param soup: HTML to parse\n",
    "    : return: List of car entries\n",
    "    \"\"\"\n",
    "    prices = soup.find_all('span', class_='price')\n",
    "    prices = [price.text[:-4].replace(' ', '') for price in prices]\n",
    "    listing_names_raw = soup.find_all('a', class_='mmmL')\n",
    "    listing_names = [listing.text for listing in listing_names_raw]\n",
    "    listing_url = [listing['href'][2:] for listing in listing_names_raw]\n",
    "    title_price_url = [list(i) for i in zip(listing_names, prices, listing_url)]\n",
    "\n",
    "    return title_price_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape the car site so we can create our dataframe\n",
    "driver = webdriver.Firefox()\n",
    "\n",
    "from selenium.webdriver.firefox.options import Options as FirefoxOptions\n",
    "\n",
    "# options = FirefoxOptions()\n",
    "# options.add_argument(\"--headless\") # This is used so no visible browser is opened\n",
    "# driver = webdriver.Firefox(options=options)\n",
    "\n",
    "# Create a list for all cars\n",
    "all_cars = []\n",
    "\n",
    "# Go to page\n",
    "driver.get(\"https://www.mobile.bg/obiavi/avtomobili-dzhipove/honda/accord\")\n",
    "\n",
    "# Generate a BeautifulSoup instance from the current page\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "total_pages = soup.find('span', class_='pageNumbersInfo')\n",
    "total_pages = int(total_pages.b.text[total_pages.b.text.rindex(' ')+1:])\n",
    "\n",
    "# Iterate all pages and pull the name, pricing and url of each offer\n",
    "for page in range(1, total_pages + 1):\n",
    "    print(f\"Scraping page {page}\")\n",
    "    title_price_url = scrape_cars(soup)\n",
    "    all_cars.extend(title_price_url)\n",
    "\n",
    "    if page != total_pages:\n",
    "        try:\n",
    "            next_button =  driver.find_element(By.XPATH, f\"//a[text()='{page + 1}']\")\n",
    "            next_button.send_keys(Keys.RETURN)\n",
    "        except NoSuchElementException as e:\n",
    "            next_button =  driver.find_element(By.XPATH, f\"//a[text()='Напред']\")\n",
    "            next_button.send_keys(Keys.RETURN)\n",
    "\n",
    "    time.sleep(0.5)\n",
    "    print(f\"Cars scraped: {len(all_cars)}\")\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cars_list = [{\"title\": name, \"price\": int(price) if price.isdigit() else price, \"url\": link} for name, price, link in all_cars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_cars_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cars_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over all cars and add supplementary parameters from offer\n",
    "for car in all_cars_list:\n",
    "    url = requests.get(f\"https://{car['url']}\")\n",
    "    url.encoding = 'windows-1251' #  Needed as the site contains Unicode characters\n",
    "    htmltext = url.text\n",
    "    soup = BeautifulSoup(htmltext)\n",
    "    data = soup.find('ul', class_='dilarData')\n",
    "    split_data = [i.text for i in data if i != '\\n']\n",
    "    for param in range(0, len(split_data), 2):\n",
    "        car[split_data[param]] = split_data[param + 1]\n",
    "    print(f\"Processing offer: {car['title']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cars_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_column', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_seq_items', None)\n",
    "pd.set_option('display.max_colwidth', 500)\n",
    "pd.set_option('expand_frame_repr', True)\n",
    "\n",
    "df = pd.DataFrame(all_cars_list)\n",
    "df = df.rename(columns={'title': 'Заглавие', 'price':'Цена', 'url':'Линк към обява'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# <p class=\"text\">Thank you for your time!</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
